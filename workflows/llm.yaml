apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: text-classification-
spec:
  entrypoint: main
  templates:
    - name: main
      inputs:
        parameters:
          - name: labels
          - name: model
          - name: input_file_name
          - name: parallelism
      steps:
        - - name: split-process
            template: process-file
            arguments:
              parameters:
                - name: labels
                  value: "{{inputs.parameters.labels}}"
                - name: model
                  value: "{{inputs.parameters.model}}"
                - name: input_file_name
                  value: "{{inputs.parameters.input_file_name}}"
                - name: chunk_index
                  value: "{{item}}"
                - name: total_chunks
                  value: "{{inputs.parameters.parallelism}}"
            withSequence:
              count: "{{inputs.parameters.parallelism}}"
        - - name: combine-results
            template: combine-chunks
            arguments:
              parameters:
                - name: input_file_name
                  value: "{{inputs.parameters.input_file_name}}"
                - name: total_chunks
                  value: "{{inputs.parameters.parallelism}}"
    
    - name: process-file
      inputs:
        parameters:
          - name: labels
            value: "{{labels}}"
          - name: model
            value: "{{model}}"
          - name: input_file_name
            value: "{{input_file_name}}"
          - name: chunk_index
            value: "{{chunk_index}}"
          - name: total_chunks
            value: "{{total_chunks}}"
        artifacts:
          - name: input-file
            path: /data/input.csv
            s3:
              endpoint: minio.argo.svc.cluster.local:9000
              bucket: "inputs"
              key: "input/{{inputs.parameters.input_file_name}}"
              insecure: true
              accessKeySecret:
                name: my-minio-cred
                key: accesskey
              secretKeySecret:
                name: my-minio-cred
                key: secretkey
      container:
        image: ollama/ollama:latest
        command: [bash, -c]
        resources:
          limits:
            cpu: "4"
            memory: "8Gi"
          requests:
            cpu: "2"
            memory: "4Gi"
        args:
          - |
            apt update && apt install -y curl
            ollama serve &
            sleep 3
            ollama pull {{inputs.parameters.model}}
            curl -LsSf https://astral.sh/uv/install.sh | sh
            source $HOME/.local/bin/env
            mkdir test
            cd test
            uv init
            uv venv
            source .venv/bin/activate
            uv add langchain
            uv add langchain-ollama
            uv add pandas
            uv add tqdm
            cat > test.py << 'SCRIPT_END'
            import pandas as pd
            import os
            import json
            import re
            from tqdm import tqdm
            from langchain_ollama import ChatOllama
            from langchain_core.messages import HumanMessage, SystemMessage
            from langchain_core.output_parsers import StrOutputParser

            def extract_label_value(text, labels_list):
                """Extract label value from response."""
                text = text.strip()
                
                try:
                    data = json.loads(text)
                    if isinstance(data, dict) and 'label' in data:
                        return data['label']
                    elif isinstance(data, str):
                        return data.strip()
                except json.JSONDecodeError:
                    pass
                
                try:
                    json_text = text.replace("'", '"')
                    data = json.loads(json_text)
                    if isinstance(data, dict) and 'label' in data:
                        return data['label']
                except json.JSONDecodeError:
                    pass
                
                patterns = [
                    r'label.*?:.*?"([^"]+)"',
                    r"label.*?:.*?'([^']+)'",
                    r'label.*?"([^"]+)"',
                    r"label.*?'([^']+)'",
                ]
                
                for pattern in patterns:
                    match = re.search(pattern, text, re.IGNORECASE)
                    if match:
                        return match.group(1).strip()
                
                for label in labels_list:
                    if label.lower() in text.lower():
                        return label
                
                return text.strip()

            def create_system_prompt(labels_str):
                """Create a system prompt for text classification."""
                return (
                    f"You are a text classification assistant. Classify the text into EXACTLY ONE of these categories: {labels_str}. "
                    f"Respond with ONLY the category name, nothing else."
                )

            input_df = pd.read_csv('/data/input.csv')
            labels_str = "{{inputs.parameters.labels}}"
            labels_list = [label.strip() for label in labels_str.split(',')]
            model_name = "{{inputs.parameters.model}}"
            chunk_index = int("{{inputs.parameters.chunk_index}}")
            total_chunks = int("{{inputs.parameters.total_chunks}}")
            system_prompt = create_system_prompt(labels_str)

            llm = ChatOllama(
                model=model_name,
                base_url="http://localhost:11434",
                temperature=0.0,
            )
            parser = StrOutputParser()

            chunk_size = len(input_df) // total_chunks
            start_idx = chunk_index * chunk_size
            if chunk_index == total_chunks - 1:
                end_idx = len(input_df)
            else:
                end_idx = start_idx + chunk_size
            
            input_df_chunk = input_df.iloc[start_idx:end_idx].copy()
            
            print(f"Processing chunk {chunk_index + 1}/{total_chunks}")
            print(f"Rows: {start_idx} to {end_idx} ({len(input_df_chunk)} rows)")
            print(f"Total dataset: {len(input_df)} rows")

            results = []
            print(f"Processing {len(input_df_chunk)} texts with model: {model_name}")
            print(f"Available labels: {labels_list}")
            print(f"Using LangChain with Ollama")

            for idx, row in tqdm(input_df_chunk.iterrows(), total=len(input_df_chunk), desc="Classifying texts"):
                text = str(row[input_df.columns[0]])
                
                try:
                    messages = [
                        SystemMessage(content=system_prompt),
                        HumanMessage(content=text)
                    ]
                    response = llm.invoke(messages)
                    raw_output = parser.invoke(response)
                    label = extract_label_value(raw_output, labels_list)
                    results.append(label)
                    
                    if (idx + 1) % 10 == 0:
                        print(f"Processed {idx + 1}/{len(input_df_chunk)} texts")
                        
                except Exception as e:
                    print(f"Error processing text at index {idx}: {str(e)}")
                    results.append("ERROR")

            input_df_chunk['label'] = results
            input_df_chunk.rename(columns={input_df_chunk.columns[0]: 'text'}, inplace=True)

            print("\n=== Chunk Classification Results ===")
            print(f"Chunk {chunk_index + 1}/{total_chunks}")
            print(f"Total samples in chunk: {len(input_df_chunk)}")
            print(f"Unique labels: {input_df_chunk['label'].nunique()}")
            print(f"Label distribution:")
            label_counts = input_df_chunk['label'].value_counts()
            for label, count in label_counts.items():
                percentage = (count / len(input_df_chunk)) * 100
                print(f"  {label}: {count} ({percentage:.2f}%)")

            input_df_chunk.to_csv('/data/output.csv', index=False)
            print(f"\nChunk results saved to /data/output.csv")
            SCRIPT_END
            python test.py
      outputs:
        artifacts:
          - name: result-file
            path: /data/output.csv
            s3:
              endpoint: minio.argo.svc.cluster.local:9000
              bucket: "inputs"
              key: "output/{{inputs.parameters.input_file_name}}_chunk_{{inputs.parameters.chunk_index}}.csv"
              insecure: true
              accessKeySecret:
                name: my-minio-cred
                key: accesskey
              secretKeySecret:
                name: my-minio-cred
                key: secretkey
            archive:
              none: {}
    
    - name: combine-chunks
      inputs:
        parameters:
          - name: input_file_name
          - name: total_chunks
      container:
        image: python:3.11-slim
        command: [bash, -c]
        args:
          - |
            pip install pandas minio
            
            cat > combine.py << 'COMBINE_END'
            import pandas as pd
            import glob
            import os
            from minio import Minio
            
            minio_client = Minio(
                "minio.argo.svc.cluster.local:9000",
                access_key=os.environ.get("MINIO_ACCESS_KEY", "admin"),
                secret_key=os.environ.get("MINIO_SECRET_KEY", "password"),
                secure=False
            )
            
            input_file_name = "{{inputs.parameters.input_file_name}}"
            total_chunks = int("{{inputs.parameters.total_chunks}}")
            
            print(f"Combining {total_chunks} chunks for {input_file_name}")
            
            chunks = []
            for i in range(total_chunks):
                chunk_key = f"output/{input_file_name}_chunk_{i}.csv"
                local_file = f"/tmp/chunk_{i}.csv"
                try:
                    minio_client.fget_object("inputs", chunk_key, local_file)
                    df = pd.read_csv(local_file)
                    chunks.append(df)
                    print(f"Downloaded chunk {i}: {len(df)} rows")
                except Exception as e:
                    print(f"Error downloading chunk {i}: {e}")
            
            if chunks:
                combined_df = pd.concat(chunks, ignore_index=True)
                print(f"\n=== Combined Results ===")
                print(f"Total samples: {len(combined_df)}")
                print(f"Unique labels: {combined_df['label'].nunique()}")
                print(f"Label distribution:")
                label_counts = combined_df['label'].value_counts()
                for label, count in label_counts.items():
                    percentage = (count / len(combined_df)) * 100
                    print(f"  {label}: {count} ({percentage:.2f}%)")
                
                os.makedirs('/data', exist_ok=True)
                combined_df.to_csv('/data/combined_output.csv', index=False)
                print(f"\nCombined results saved")
            else:
                print("No chunks found to combine!")
            COMBINE_END
            
            python combine.py
      outputs:
        artifacts:
          - name: combined-file
            path: /data/combined_output.csv
            s3:
              endpoint: minio.argo.svc.cluster.local:9000
              bucket: "inputs"
              key: "output/{{inputs.parameters.input_file_name}}"
              insecure: true
              accessKeySecret:
                name: my-minio-cred
                key: accesskey
              secretKeySecret:
                name: my-minio-cred
                key: secretkey
            archive:
              none: {}